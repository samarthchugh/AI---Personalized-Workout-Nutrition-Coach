# ğŸ§  AI-powered Personalized Workout & Nutrition Coach

An end-to-end **AI-driven system** that generates **personalized workout and nutrition recommendations** based on user input â€” built to demonstrate advanced proficiency in **Machine Learning**, **MLOps**, and **Cloud Deployment**.

Hosted entirely on **AWS EC2**, the project integrates **ETL, training, and inference pipelines**, **model versioning with DVC**, **experiment tracking with MLflow**, and a **hybrid AI chatbot** capable of both retrieval and generative responses.

---

## ğŸ¯ Project Objective

The goal of this project is to showcase a **complete MLOps lifecycle**, from data ingestion to cloud deployment, while solving a real-world problem â€” helping users receive AI-based **fitness and nutrition guidance**.

**Key Highlights**

* End-to-end ML lifecycle automation (ETL â†’ Train â†’ Inference)
* Cloud-native MLOps integration with AWS
* Hybrid chatbot (retrieval + generative)
* Dockerized API deployed via CI/CD to EC2
* Data versioning and experiment tracking using DVC + MLflow

---

## ğŸ§© Features

### ğŸ‹ï¸ Personalized Recommendations

* Suggests **workouts** (type, duration, difficulty)
* Recommends **meals** (nutritional value breakdown)
* Predictions powered by ML pipelines with preprocessing (scaling, encoding, KNN imputation, label encoding)

### ğŸ’¬ Intelligent Chatbot

* Hybrid model combining **retrieval** and **generative** approaches
* Uses embeddings from `all-MiniLM-L6-v2` for context retrieval
* Generates answers via `google/gemma-2-2b-it` (Hugging Face Inference API)
* Automatically switches between retrieval and generative modes based on similarity threshold

### â˜ï¸ Full MLOps Integration

* **ETL Pipeline:** Pulls raw data from PostgreSQL (AWS RDS), performs transformations, and stores processed data in AWS S3 via DVC
* **Training Pipeline:** Fetches processed data from S3, runs DVC stages, and tracks model performance using MLflow + Dagshub
* **Inference Pipeline:** Loads latest model from S3/DVC for serving predictions through a FastAPI endpoint

### ğŸ³ Deployment & CI/CD

* **Dockerized FastAPI application** for reproducible inference environment
* **GitHub Actions** for continuous deployment to AWS EC2
* **Entrypoint script** automates data seeding, model retrieval, and API startup

---

## ğŸ—ï¸ Architecture Overview

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Synthetic CSV Data        â”‚
           â”‚ â€¢ Workouts (100)          â”‚
           â”‚ â€¢ Nutrition (100)         â”‚
           â”‚ â€¢ FAQs (20)               â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ PostgreSQL (AWS RDS)      â”‚
           â”‚ Seeded via SQLAlchemy ORM â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ ETL Pipeline              â”‚
           â”‚ Clean + Transform + DVC   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Training Pipeline          â”‚
           â”‚ MLflow + Dagshub Tracking â”‚
           â”‚ DVC for Model Versioning  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Inference Pipeline        â”‚
           â”‚ FastAPI on AWS EC2        â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Tech Stack

| Category                | Technologies                                      |
| ----------------------- | ------------------------------------------------- |
| **Language**            | Python 3.11                                       |
| **Data Processing**     | Pandas, NumPy, Scikit-learn                       |
| **Database**            | PostgreSQL (AWS RDS), SQLAlchemy ORM              |
| **Modeling**            | Scikit-learn, Transformers, Sentence-Transformers |
| **Model Versioning**    | DVC with AWS S3 remote                            |
| **Experiment Tracking** | MLflow + Dagshub                                  |
| **Web Framework**       | FastAPI, Uvicorn                                  |
| **Cloud Services**      | AWS EC2, S3, RDS                                  |
| **Containerization**    | Docker                                            |
| **CI/CD**               | GitHub Actions                                    |
| **Others**              | boto3, awscli, python-dotenv                      |

---

## ğŸ§® Data & Model Management

* **Synthetic data** generated via ChatGPT:

  * 100 Nutrition entries
  * 100 Workout entries
  * 20 FAQs
* **Database:** Seeded into AWS RDS PostgreSQL using SQLAlchemy ORM
* **Models & Processed Data:** Versioned using **DVC** and stored in AWS S3
* **Experiments:** Tracked with **MLflow** integrated with **Dagshub**

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/samarthchugh/AI---Personalized-Workout-Nutrition-Coach.git
cd AI---Personalized-Workout-Nutrition-Coach
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
source venv/bin/activate     # macOS/Linux
venv\Scripts\activate        # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Environment

Create a `.env` file in the root directory:

```
AWS_ACCESS_KEY_ID=your_aws_key
AWS_SECRET_ACCESS_KEY=your_aws_secret
AWS_DEFAULT_REGION=your_region
DATABASE_URL=postgresql+psycopg2://<user>:<password>@<rds-endpoint>/<db>
```

### 5ï¸âƒ£ Initialize DVC

```bash
dvc init
dvc pull    # fetches versioned data/models from S3
```

### 6ï¸âƒ£ Run Locally

```bash
uvicorn src.API.main:app --host 0.0.0.0 --port 8000
```

Access interactive API docs at
ğŸ‘‰ **[http://localhost:8000/docs](http://localhost:8000/docs)**

---

## ğŸ³ Docker Deployment

```bash
docker build -t ai-personal-coach .
docker run -p 8000:8000 ai-personal-coach
```

**Entrypoint Script (`entrypoint.sh`)**

* Seeds data into RDS
* Pulls latest model from DVC (S3)
* Starts FastAPI server

---

## ğŸ”„ CI/CD Workflow

* Triggered on each **push/commit to main**
* GitHub Actions:

  * Builds Docker image
  * Connects via SSH to AWS EC2
  * Pulls latest code and restarts container
* Ensures **seamless production updates** with no downtime

---

## ğŸ§¾ Key Files & Modules

| File / Directory                      | Description                                    |
| ------------------------------------- | ---------------------------------------------- |
| `scripts/seed_data.py`                | Seeds CSV data into PostgreSQL RDS             |
| `src/pipelines/etl_pipeline.py`       | Extract â†’ Transform â†’ Load pipeline            |
| `src/pipelines/training_pipeline.py`  | Trains and tracks models                       |
| `src/pipelines/inference_pipeline.py` | Runs inference from trained models             |
| `src/chatbot/`                        | Hybrid chatbot models (retrieval + generative) |
| `dvc.yaml`                            | Pipeline stages for reproducibility            |
| `entrypoint.sh`                       | Automates EC2 container startup                |

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
See the [LICENSE](./LICENSE) file for details.

---

## ğŸ‘¤ Author

**ğŸ‘¨â€ğŸ’» Samarth Chugh**
MLOps & Machine Learning Engineer
ğŸ“ Hosted on **AWS EC2**
ğŸ”— [GitHub Profile](https://github.com/samarthchugh)

---

### â­ If you find this project inspiring, please give it a star â€” it helps others discover it and supports future improvements!
